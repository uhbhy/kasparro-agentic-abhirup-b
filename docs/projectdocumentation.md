# Project Documentation

## Problem Statement
Design a modular, agentic system that reads a single product JSON and produces machine-readable content pages (FAQ, product page, comparison) using agent-based orchestration.

## Solution Overview
We implement a small orchestration graph:
- **ParserAgent:** parse/normalize input.
- **QAAgent:** generate categorized questions and model-driven answers using LangChain/OpenAI.
- **ContentAgent:** produce product page JSON via reusable blocks + LangChain for polish.
- **CompareAgent:** synthesize a structured fictional competitor and compare via logic blocks.
- **Orchestrator:** DAG coordinator that runs agents and writes JSON outputs.

This uses LangChain for all LLM calls (no one-off GPT wrappers), reusable blocks for transforms and comparisons, and a small template engine to enforce structured output. Agents have single responsibilities and pass plain data (no hidden globals).

## Scopes & Assumptions
- Only the provided product dataset is used. No external data fetching.
- OpenAI is used as the LLM via LangChain.
- The fictional product B is generated by the model but constrained to return a strict JSON object.
- Minimal external dependencies for clarity.

## System Design
- Agents are independent classes (single responsibility).
- Orchestration graph: `Parser -> QA (questions+answers) -> Content -> Compare -> outputs`.
- Data flow is explicit: each agent accepts and returns dicts.
- Template engine enforces shape and basic validation.

## How this meets evaluator requirements
- **LangChain used** for question generation and content polishing.
- **Agent boundaries** are explicit and testable.
- **Reusable logic blocks** exist under `src/blocks/`.
- **Template engine** under `src/templates/`.
- **Outputs** are clean JSON files in `outputs/`.

## Files of interest
- `src/agents/*.py` - agents
- `src/blocks` - reusable blocks
- `src/templates` - template definitions and engine
- `docs/projectdocumentation.md` - this file
